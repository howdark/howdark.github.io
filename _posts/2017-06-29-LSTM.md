---
title: "LSTM Algorithm"
author: "Seongbong Kim"
date: 2017-06-29 23:36:00 +0900
categories: jekyll update deep-learning
permalink: /blog/:title
comments: true
---


## RNN Algorithm - 1 (Simple RNN)

#### 1. LSTM의 특성

-   Basic RNN과 같이 Cell을 반복 사용하는 점은 동일
-   이전 Output과 Cell State 모두 다음 LSTM Cell의 입력으로 사용되는 점이 다름(Self-Loop)
-   Cell State는 약간의 선형 작용만 일으키며 다음 State로 전달되기 때문에 Vanishing 문제를 해결
-   Cell 내부에 Gate Unit을 구성하고, input/output/cell state를 조정함


<center>![LSTM1](/assets/DL_algorithm/RNN/LSTM1.png)</center>

<br>
#### 2. Gate 소개

<br>
###### Forget Gate

-   이전 Output($h_{t-1}$)과 input($x_t$)을 기반으로 이전 State($S_{t-1}$)를 얼마나 기억할 지를 결정하는 Gate
-   이전 State와 곱하여 현재 State의 Base를 산출하는 용도로 사용됨
-   $Sigmoid$ 함수를 사용하므로 0~1 사이의 값을 반환
-   반환되는 값은 0은 완전 삭제, 1은 완전 보존을 의미


<center>![LSTM2](/assets/DL_algorithm/RNN/LSTM2.png)</center>

<br>
###### Input Gate & Candidate Layer

-   어떤 Input을 얼마나 State에 반영할 지 결정하는 부분
-   Input Gate ($sigmoid$) : 이전 Output($h_{t-1}$)과 input($x_t$)을 기반으로, 어떤 값을 update할 지 결정하는 Gate
-   Candidate Layer ($tanh$) : 이전 Output($h_{t-1}$)과 input($x_t$)을 기반으로, State에 저장할 값을 정하는 Layer
-   Input Gate(0~1)와 Candidate Layer(-1~1)를 곱해서, 어떤 값을 얼마나 저장할지 확정한 후, State에 합산

<center>![LSTM3](/assets/DL_algorithm/RNN/LSTM3.png)</center>

<br>
###### Cell State Update

-   Forget Gate와 Input Gate $\times$ Candidate Layer로 확정된 내용을 적용하여 State를 Update
    1.  이전 State와 Forget Gate의 출력을 곱하여, 이전 State의 보존 정도를 결정
    2.  Input Gate의 출력과 Candidate Layer를 곱하여 결정된, 현재 입력의 적용 정도를 합산 반영
-   위 두 연산을 통해 현재 State Update를 완료


<center>![LSTM4](/assets/DL_algorithm/RNN/LSTM4.png)</center>

<br>
###### Output Gate

-   Output을 연산하는 부분
-   Output Gate : 현재 Cell State에서 어느 Part를 출력할 지 결정하는 Gate
-   Output($h_t$) : 현재 Cell State를 $tanh$ 연산을 통과시킨 후, Output Gate와 곱하여 선택 Part를 출력


<center>![LSTM5](/assets/DL_algorithm/RNN/LSTM5.png)</center>

<br>
#### 3. 그 외 RNN 관련 Algorithm & Architecture

-   Optimization for Long-Term Dependencies
    -   Clipping the gradient : it helps exploding gradient but doesn’t help vanishing.

-   Explicit Memory 기법
    -   Memory Networks
    -   Neural Turing Machine
    -   Contents-based addressing
    -   Location-based addressing
    -   Attention mechanism

-   다른 RNN Architecture
    -   Bi-directional RNN
    -   Recursive Neural Network
    -   Continuous-time RNN
    -   Hierarchical RNN
