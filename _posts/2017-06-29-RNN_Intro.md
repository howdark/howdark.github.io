---
title: "RNN Introduction"
author: "Seongbong Kim"
date: 2017-06-29 23:33:00 +0900
categories: jekyll update deep-learning
permalink: /blog/:title
comments: true
use_math: true
---

## Why RNN?

### 1. RNN (Recurrent Neural Networks)

-   순차적인 정보를 처리하기 위해 탄생함
-   기존에 발생한 사건으로부터 지속성을 가지고 문제를 바라보기 위한 알고리즘
-   효율성 : Weight가 여러 time step에 걸쳐 공유됨
-   실제로 잘 작동함!  -  몇몇 음성과 자연어 처리에서 최고 성능을 내는 알고리즘
-   Recurrent라는 단어가 붙은 이유
    1.   하나의 sequence에 속한 모든 요소(element)마다 동일한 태스크(또는 연산)를 반복 적용
          <br>
          >예시) 문장(sequence)의 모든 단어(element), 시계열 데이터(sequence)의 모든 데이터(element)

    2.   Output은 이전 계산 결과에 영향을 받음 (일종의 Memory 정보 전달)


### 2. 기존 신경망의 한계

-   기존 신경망은 모든 입력과 출력이 영향을 주지 않고 독립적이라 판단
-   하지만 실생활 문제들은 이전에 발생한 사건을 참조하는 것이 도움이 될 때가 많음
-   예시
    -   과거 전기 사용량을 바탕으로 올해 전기 사용량 예측
    -   이전 질문에 대한 대답
    -   문장, 문단 내 문맥의 이해

<br>
## RNN 활용 예

RNN은 sequence data를 모델링 하므로, Time Series, Natural Language, Speech 등에 활용.
또한 non-sequence data를 sequence data로 변환하여 활용 가능 (이미지의 pixel을 sequence로..)

![RNN_Examples](/assets/DL_algorithm/RNN/RNN_Examples.png)

<br>
## RNN이란?
[그림/내용 인용 : John Canny. Fall 2016. Lecture 10: Recurrent Networks, LSTMs and Applications.](https://bcourses.berkeley.edu/courses/1453965/files/69726080/download?verifier=nM037oSiGKxx2NvncaMvYXLKrhL9nj6vvohQpHIm&wrap=1)
-   <b>R</b>ecurrent <b>N</b>eural <b>N</b>etworks의 약자
-   주기와 시간 개념을 도입
-   순차적인 데이터를 처리하도록 디자인 되었고, 순차적인 출력을 생성할 수 있음

![RNN1](/assets/DL_algorithm/RNN/RNN1.png)

-   Sequential Data : $x_1, x_2, \cdots, x_n$
-   Sequential Output : $y_1, y_2, \cdots, y_m$
-   Sequential State : $h_1, h_2, \cdots, h_n$

<br>
## Recurrent라는 단어가 붙은 이유

-   하나의 sequence $(x_1, \cdots, x_n)$에 동일한 연산을 반복 적용
-   Output$(y_1, \cdots, y_m)$은 이전 연산 결과 $(h_1, \cdots, h_n)$에 영향을 받음 (일종의 Memory 정보 전달)

![RNN2](/assets/DL_algorithm/RNN/RNN2.png)

<br>

## Neural Networks인 이유

-   하나의 Time Step만 놓고 보면 Neural Networks, 다만 $h_t$가 다음 Step의 Input이어서 Recurrent
-   $h_t$ 의 size는 모델 디자인 할 때 결정

![RNN3](/assets/DL_algorithm/RNN/RNN3.png)


<br>
## RNN Structure

-   RNN도 Neural Network이기 때문에 Layer를 쌓을 수 있음
-   각 Layer 내에서는 같은 Parameter를 사용해 Recurrent 동작
-   Layer 간 Parameter는 서로 다름

![RNN4](/assets/DL_algorithm/RNN/RNN4.png)

<br>
## RNN 모델 디자인

-   입력/출력 별 모델 디자인을 구상하는데 유연성을 가짐

![RNN5](/assets/DL_algorithm/RNN/RNN5.png)

[인용:전태균님 깃허브 내 PPT](https://github.com/tgjeon/TensorFlow-Tutorials-for-Time-Series/raw/master/KSC2016%20-%20Recurrent%20Neural%20Networks.pptx)


## 기타 참고 사항
### 1. CNN과의 비교

|               | Convolutional | Recurrent |
| ------------- | ------------- | ------------- |
| 특화분야 | 표 형태의 값 처리(이미지 등) | 순차적으로 발생하는 값 처리 |
| 확장성       | 큰 이미지, 가변 크기 이미지 등 | 타 network들 대비 훨씬 긴 순차 데이터 적용 가능<br>(기본 RNN은 실제로는 비교적 짧은 sequence만 효과적으로 처리) |



<br>

### 2. 특징 - Parameter 공유

-   순차적인 데이터를 학습할 때 RNN은 동일한 연산 유닛을 반복하여 사용하기 때문에, 연산 유닛 안의 Parameter가 공유됨
-   만약 기존의 NN 방식에서 순차적인 과거의 데이터를 모두 반영한다면, 각 time index($\cdots, t-2, t-1, t $)별 Parameter를 각각 학습해야 함

-   예를 들어 화자가 Nepal에 방문한 년도를 추출하는 문제의 접근 방법을 본다면,
      > A : I went to Nepal in <font color="red"><u>2009</u></font>.
      > B : In <font color="red"><u>2009</u></font>, I went to Nepal.

-   기존 Feed Forward NN은 position 별 "2009"가 발생할 확률을 학습을 해야하는 단점이 있음
-   반면 RNN은 "2009"가 발생할 확률을 동일한 연산 유닛으로 학습하므로 Parameter가 공유됨
-   Parameter 공유를 통해서 서로 다른 길이의 예제에 확장/적용하고, 일반화가 가능함


|Model|Method|
|--- | --- |
|Fully Connected Feed Forward|- input position마다 weight를 설정<br> - 각 position에서의 모든 rule을 학습|
| Recurrent Neural Networks | - time step에 대해 동일한 weight를 공유|


-   만약 Time sequence에 대해 기존 NN 방식으로 각 index 별 Parameter를 가져간다면,
    -   학습 과정에 보지 못한 sequence 길이에 대해 일반화를 할 수 없음
        -   RNN : $ h^{(t)} = f(h^{(t-1)},x^{(t)})$
        -   기존 NN : $h^{(t)} = g(x^{(t)}, x^{(t-1)}, x^{(t-2)}, \cdots, x^{(2)}, x^{(1)})$
        -   기존 NN은 매 $t$ 시점에 과거의 모든 sequence 데이터를 input으로 받아야 하기 때문에, 사전에 학습되지 않은 sequence 길이에 대해서 input을 받기가 어렵기 때문
    -   시간에 따라 다른 sequence 길이나 다른 위치에 대해 통계적 힘(?)을 공유할 수 없음

<br>
